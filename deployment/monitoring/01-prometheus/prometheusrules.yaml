# PrometheusRules for Alerting
# Matrix/Synapse Production Deployment
#
# Comprehensive alerting rules for:
# - Synapse main and workers
# - Synapse LI instance
# - PostgreSQL replication
# - Redis Sentinel
# - MinIO storage
# - HAProxy routing
# - LI sync system

---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: matrix-alerts
  namespace: matrix
  labels:
    prometheus: kube-prometheus
spec:
  groups:
    # ========================================================================
    # SYNAPSE MAIN INSTANCE ALERTS
    # ========================================================================
    - name: synapse-main
      interval: 30s
      rules:
        - alert: SynapseMainDown
          expr: up{job="synapse", app_kubernetes_io_instance="main"} == 0
          for: 2m
          labels:
            severity: critical
            component: synapse
          annotations:
            summary: "Synapse main process is down"
            description: "Synapse main instance {{ $labels.pod }} has been down for more than 2 minutes. This is a critical failure."

        - alert: SynapseHighSyncLatency
          expr: histogram_quantile(0.95, rate(synapse_http_server_response_time_seconds_bucket{servlet="sync"}[5m])) > 0.5
          for: 10m
          labels:
            severity: warning
            component: synapse
          annotations:
            summary: "High Synapse /sync latency"
            description: "95th percentile /sync latency is {{ $value }}s (threshold: 500ms) on {{ $labels.pod }}"

        - alert: SynapseHighEventPersistLag
          expr: synapse_storage_events_persisted_by_source_type_total > 1000
          for: 5m
          labels:
            severity: warning
            component: synapse
          annotations:
            summary: "High event persist lag"
            description: "Event persist lag is {{ $value }}ms (threshold: 1000ms) on {{ $labels.pod }}"

        - alert: SynapseHighCPU
          expr: rate(process_cpu_seconds_total{job="synapse"}[5m]) > 0.8
          for: 15m
          labels:
            severity: warning
            component: synapse
          annotations:
            summary: "Synapse high CPU usage"
            description: "Synapse instance {{ $labels.pod }} is using {{ $value | humanizePercentage }} CPU for more than 15 minutes"

        - alert: SynapseHighMemory
          expr: process_resident_memory_bytes{job="synapse"} / 1024 / 1024 / 1024 > 3.5
          for: 10m
          labels:
            severity: warning
            component: synapse
          annotations:
            summary: "Synapse high memory usage"
            description: "Synapse instance {{ $labels.pod }} is using {{ $value }}GB of memory (threshold: 3.5GB)"

        - alert: SynapseConnectionPoolExhausted
          expr: synapse_db_pool_connections_in_use / synapse_db_pool_connections_max > 0.9
          for: 5m
          labels:
            severity: critical
            component: synapse
          annotations:
            summary: "Synapse database connection pool nearly exhausted"
            description: "Synapse {{ $labels.pod }} is using {{ $value | humanizePercentage }} of database connections"

    # ========================================================================
    # SYNAPSE WORKERS ALERTS
    # ========================================================================
    - name: synapse-workers
      interval: 30s
      rules:
        - alert: SynapseWorkerDown
          expr: up{job="synapse", app_kubernetes_io_component="worker"} == 0
          for: 5m
          labels:
            severity: warning
            component: synapse-workers
          annotations:
            summary: "Synapse worker is down"
            description: "Worker {{ $labels.pod }} (type: {{ $labels.worker_type }}) has been down for more than 5 minutes"

        - alert: SynapseSynchrotronHighLoad
          expr: avg(rate(synapse_http_server_requests_total{worker_type="synchrotron"}[5m])) > 100
          for: 10m
          labels:
            severity: warning
            component: synapse-workers
          annotations:
            summary: "Synchrotron workers under high load"
            description: "Synchrotron workers are handling {{ $value }} requests/sec (threshold: 100 req/s). Consider scaling."

        - alert: SynapseMediaWorkerHighLoad
          expr: avg(rate(synapse_http_server_requests_total{worker_type="media"}[5m])) > 50
          for: 10m
          labels:
            severity: warning
            component: synapse-workers
          annotations:
            summary: "Media workers under high load"
            description: "Media workers are handling {{ $value }} requests/sec (threshold: 50 req/s). Consider scaling."

        - alert: SynapseWorkerReplicationLag
          expr: synapse_replication_tcp_command_queue_size > 500
          for: 5m
          labels:
            severity: warning
            component: synapse-workers
          annotations:
            summary: "Worker replication lag detected"
            description: "Worker {{ $labels.pod }} has replication queue size of {{ $value }} (threshold: 500)"

    # ========================================================================
    # SYNAPSE LI INSTANCE ALERTS
    # ========================================================================
    - name: synapse-li
      interval: 30s
      rules:
        - alert: SynapseLIDown
          expr: up{job="synapse", matrix_instance="li"} == 0
          for: 10m
          labels:
            severity: warning
            component: synapse-li
          annotations:
            summary: "Synapse LI instance is down"
            description: "LI instance {{ $labels.pod }} has been down for more than 10 minutes"

        - alert: SynapseLIHighReplicationLag
          expr: (cnpg_pg_replication_lag{cnpg_io_cluster="matrix-postgresql-li"} > 300)
          for: 5m
          labels:
            severity: critical
            component: synapse-li
          annotations:
            summary: "LI PostgreSQL replication lag is high"
            description: "LI database replication lag is {{ $value }}s (threshold: 300s / 5min). Data sync may be delayed."

    # ========================================================================
    # POSTGRESQL ALERTS
    # ========================================================================
    - name: postgresql
      interval: 30s
      rules:
        - alert: PostgreSQLDown
          expr: up{job="postgresql"} == 0
          for: 2m
          labels:
            severity: critical
            component: postgresql
          annotations:
            summary: "PostgreSQL instance down"
            description: "PostgreSQL pod {{ $labels.pod }} is down"

        - alert: PostgreSQLReplicationLag
          expr: cnpg_pg_replication_lag > 60
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "PostgreSQL replication lag detected"
            description: "Standby {{ $labels.pod }} is lagging {{ $value }} seconds behind primary"

        - alert: PostgreSQLTooManyConnections
          expr: cnpg_pg_stat_database_numbackends / cnpg_pg_settings_max_connections > 0.8
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "PostgreSQL has too many connections"
            description: "Database {{ $labels.datname }} on {{ $labels.pod }} is using {{ $value | humanizePercentage }} of max connections"

        - alert: PostgreSQLLongRunningTransaction
          expr: cnpg_backends_max_tx_duration_seconds > 600
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "Long-running PostgreSQL transaction"
            description: "Transaction on {{ $labels.pod }} has been running for {{ $value }}s (threshold: 600s / 10min)"

        - alert: PostgreSQLDeadlocks
          expr: rate(cnpg_pg_stat_database_deadlocks[5m]) > 0.1
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "PostgreSQL deadlocks detected"
            description: "Database {{ $labels.datname }} on {{ $labels.pod }} is experiencing {{ $value }} deadlocks/sec"

        - alert: PostgreSQLBackupFailing
          expr: (cnpg_pg_stat_archiver_last_failed_time - cnpg_pg_stat_archiver_last_archived_time) > 1
          for: 30m
          labels:
            severity: critical
            component: postgresql
          annotations:
            summary: "PostgreSQL backup is failing"
            description: "WAL archiving is failing on {{ $labels.pod }}. Backups may be incomplete."

        - alert: PostgreSQLXIDWraparound
          expr: cnpg_pg_database_xid_age > 1000000000
          for: 1h
          labels:
            severity: critical
            component: postgresql
          annotations:
            summary: "PostgreSQL XID wraparound approaching"
            description: "Database {{ $labels.datname }} on {{ $labels.pod }} has XID age of {{ $value }} (threshold: 1B). VACUUM needed urgently."

    # ========================================================================
    # REDIS SENTINEL ALERTS
    # ========================================================================
    - name: redis
      interval: 30s
      rules:
        - alert: RedisDown
          expr: up{job=~"redis.*"} == 0
          for: 2m
          labels:
            severity: critical
            component: redis
          annotations:
            summary: "Redis instance down"
            description: "Redis pod {{ $labels.pod }} is down"

        - alert: RedisSentinelMasterDown
          expr: redis_sentinel_masters{master_status="down"} > 0
          for: 2m
          labels:
            severity: critical
            component: redis
          annotations:
            summary: "Redis Sentinel master is down"
            description: "Redis master {{ $labels.master_name }} is reported down by Sentinel"

        - alert: RedisHighMemoryUsage
          expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
          for: 10m
          labels:
            severity: warning
            component: redis
          annotations:
            summary: "Redis high memory usage"
            description: "Redis {{ $labels.pod }} is using {{ $value | humanizePercentage }} of max memory"

        - alert: RedisConnectionsExhausted
          expr: redis_connected_clients / redis_config_maxclients > 0.8
          for: 5m
          labels:
            severity: warning
            component: redis
          annotations:
            summary: "Redis connections nearly exhausted"
            description: "Redis {{ $labels.pod }} is using {{ $value | humanizePercentage }} of max client connections"

        - alert: RedisReplicationBroken
          expr: redis_connected_slaves == 0 and redis_instance_info{role="master"} == 1
          for: 5m
          labels:
            severity: critical
            component: redis
          annotations:
            summary: "Redis replication broken"
            description: "Redis master {{ $labels.pod }} has no connected slaves"

    # ========================================================================
    # MINIO ALERTS
    # ========================================================================
    - name: minio
      interval: 30s
      rules:
        - alert: MinIODown
          expr: up{job="minio"} == 0
          for: 5m
          labels:
            severity: critical
            component: minio
          annotations:
            summary: "MinIO instance down"
            description: "MinIO pod {{ $labels.pod }} is down"

        - alert: MinIOHighStorageUsage
          expr: (minio_cluster_capacity_usable_free_bytes / minio_cluster_capacity_usable_total_bytes) < 0.2
          for: 30m
          labels:
            severity: warning
            component: minio
          annotations:
            summary: "MinIO storage capacity low"
            description: "MinIO has only {{ $value | humanizePercentage }} free space remaining"

        - alert: MinIODiskOffline
          expr: minio_cluster_disk_offline_total > 0
          for: 5m
          labels:
            severity: critical
            component: minio
          annotations:
            summary: "MinIO disk offline"
            description: "{{ $value }} disk(s) are offline in MinIO cluster. Data redundancy may be compromised."

        - alert: MinIONodeOffline
          expr: minio_cluster_nodes_offline_total > 0
          for: 10m
          labels:
            severity: critical
            component: minio
          annotations:
            summary: "MinIO node offline"
            description: "{{ $value }} node(s) are offline in MinIO cluster"

        - alert: MinIOHighRequestErrors
          expr: rate(minio_s3_requests_errors_total[5m]) > 0.05
          for: 10m
          labels:
            severity: warning
            component: minio
          annotations:
            summary: "MinIO high request error rate"
            description: "MinIO is experiencing {{ $value }} errors/sec (threshold: 0.05 errors/sec)"

    # ========================================================================
    # HAPROXY ALERTS
    # ========================================================================
    - name: haproxy
      interval: 30s
      rules:
        - alert: HAProxyDown
          expr: up{job="haproxy"} == 0
          for: 2m
          labels:
            severity: critical
            component: haproxy
          annotations:
            summary: "HAProxy is down"
            description: "HAProxy pod {{ $labels.pod }} is down"

        - alert: HAProxyBackendDown
          expr: haproxy_backend_up == 0
          for: 5m
          labels:
            severity: critical
            component: haproxy
          annotations:
            summary: "HAProxy backend is down"
            description: "HAProxy backend {{ $labels.backend }} is down"

        - alert: HAProxyHighErrorRate
          expr: rate(haproxy_backend_http_responses_total{code="5xx"}[5m]) / rate(haproxy_backend_http_responses_total[5m]) > 0.05
          for: 5m
          labels:
            severity: warning
            component: haproxy
          annotations:
            summary: "HAProxy high 5xx error rate"
            description: "Backend {{ $labels.backend }} has {{ $value | humanizePercentage }} 5xx error rate"

    # ========================================================================
    # LI SYNC SYSTEM ALERTS
    # ========================================================================
    - name: sync-system
      interval: 60s
      rules:
        - alert: SyncSystemMediaJobFailing
          expr: kube_job_status_failed{job_name=~"sync-system-media.*"} > 0
          for: 15m
          labels:
            severity: critical
            component: sync-system
          annotations:
            summary: "LI media sync job is failing"
            description: "Media sync CronJob {{ $labels.job_name }} has failed. LI media may be out of sync."

        - alert: SyncSystemReplicationLagHigh
          expr: (cnpg_pg_replication_lag{cnpg_io_cluster="matrix-postgresql-li"} > 600)
          for: 10m
          labels:
            severity: critical
            component: sync-system
          annotations:
            summary: "LI database replication severely lagged"
            description: "LI replication lag is {{ $value }}s (threshold: 600s / 10min). Sync system may have issues."

        - alert: SyncSystemMediaJobNotRunning
          expr: (time() - kube_job_status_completion_time{job_name=~"sync-system-media.*"}) > 1800
          for: 5m
          labels:
            severity: warning
            component: sync-system
          annotations:
            summary: "LI media sync job hasn't run recently"
            description: "Last media sync was {{ $value | humanizeDuration }} ago (expected: every 15 minutes)"

    # ========================================================================
    # KEY_VAULT ALERTS
    # ========================================================================
    - name: key-vault
      interval: 30s
      rules:
        - alert: KeyVaultDown
          expr: up{job="key-vault"} == 0
          for: 5m
          labels:
            severity: critical
            component: key-vault
          annotations:
            summary: "key_vault is down"
            description: "key_vault pod {{ $labels.pod }} is down. E2EE key recovery unavailable."

        - alert: KeyVaultHighErrorRate
          expr: rate(key_vault_errors_total[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
            component: key-vault
          annotations:
            summary: "key_vault high error rate"
            description: "key_vault is experiencing {{ $value }} errors/sec"

    # ========================================================================
    # NGINX INGRESS ALERTS
    # ========================================================================
    - name: nginx-ingress
      interval: 30s
      rules:
        - alert: NginxIngressDown
          expr: up{job="nginx-ingress"} == 0
          for: 2m
          labels:
            severity: critical
            component: ingress
          annotations:
            summary: "NGINX Ingress controller is down"
            description: "Ingress controller pod {{ $labels.pod }} is down"

        - alert: NginxIngressHighErrorRate
          expr: rate(nginx_ingress_controller_requests{status=~"5.."}[5m]) / rate(nginx_ingress_controller_requests[5m]) > 0.05
          for: 5m
          labels:
            severity: warning
            component: ingress
          annotations:
            summary: "NGINX Ingress high 5xx error rate"
            description: "Ingress {{ $labels.ingress }} has {{ $value | humanizePercentage }} 5xx error rate"

    # ========================================================================
    # KUBERNETES RESOURCE ALERTS
    # ========================================================================
    - name: kubernetes-resources
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total{namespace="matrix"}[15m]) > 0.1
          for: 5m
          labels:
            severity: warning
            component: kubernetes
          annotations:
            summary: "Pod is crash looping"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"

        - alert: PodNotReady
          expr: kube_pod_status_ready{condition="false", namespace="matrix"} == 1
          for: 15m
          labels:
            severity: warning
            component: kubernetes
          annotations:
            summary: "Pod not ready"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for 15 minutes"

        - alert: PersistentVolumeClaimPending
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending", namespace="matrix"} == 1
          for: 10m
          labels:
            severity: warning
            component: kubernetes
          annotations:
            summary: "PVC pending"
            description: "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is pending"
