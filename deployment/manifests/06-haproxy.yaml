# HAProxy Routing Layer for Matrix/Synapse
# Intelligent routing to specialized workers with health-aware load balancing
# Version: 2.0
# Scale-aware: Adjust replicas based on load (see SCALING-GUIDE.md)

---
# ============================================================================
# HAPROXY CONFIGMAP
# ============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: haproxy-config
  namespace: matrix
  labels:
    app: haproxy
    component: routing-layer
data:
  # Main HAProxy configuration
  haproxy.cfg: |
    # This will be populated from deployment/config/haproxy.cfg
    # Use: kubectl create configmap haproxy-config --from-file=haproxy.cfg=config/haproxy.cfg -n matrix

---
# ============================================================================
# HAPROXY DEPLOYMENT
# ============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: haproxy
  namespace: matrix
  labels:
    app: haproxy
    component: routing-layer
spec:
  # Scale based on load
  # 100 CCU: 2 replicas
  # 1K CCU: 2 replicas
  # 5K CCU: 3 replicas
  # 10K CCU: 4 replicas
  # 20K CCU: 6 replicas
  replicas: 2

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero downtime updates

  selector:
    matchLabels:
      app: haproxy
      component: routing-layer

  template:
    metadata:
      labels:
        app: haproxy
        component: routing-layer
      annotations:
        # Config hash for automatic pod restart on config change
        # Update this when haproxy.cfg changes
        config-hash: "REPLACE_WITH_CONFIG_HASH"

        # Prometheus scraping
        prometheus.io/scrape: "true"
        prometheus.io/port: "8404"
        prometheus.io/path: "/metrics"

    spec:
      # Security context for pod
      securityContext:
        runAsNonRoot: true
        runAsUser: 99  # HAProxy user
        runAsGroup: 99
        fsGroup: 99
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile:
          type: RuntimeDefault

      # Init container to validate configuration
      initContainers:
      - name: config-validator
        image: haproxy:2.8-alpine
        imagePullPolicy: IfNotPresent

        command:
          - /bin/sh
          - -c
          - |
            echo "Validating HAProxy configuration..."
            haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg
            if [ $? -eq 0 ]; then
              echo "✓ Configuration is valid"
              exit 0
            else
              echo "✗ Configuration validation failed"
              exit 1
            fi

        volumeMounts:
        - name: haproxy-config
          mountPath: /usr/local/etc/haproxy
          readOnly: true

        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL

      containers:
      - name: haproxy
        image: haproxy:2.8-alpine
        imagePullPolicy: IfNotPresent

        # Security context for container
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL

        # Ports
        ports:
        - name: http
          containerPort: 8008
          protocol: TCP
        - name: stats
          containerPort: 8404
          protocol: TCP

        # Environment variables
        env:
        - name: TZ
          value: "UTC"

        # Volume mounts
        volumeMounts:
        # HAProxy configuration
        - name: haproxy-config
          mountPath: /usr/local/etc/haproxy
          readOnly: true

        # Runtime socket directory (needed for stats socket)
        - name: run
          mountPath: /var/run

        # Temporary directories (read-only root filesystem requires these)
        - name: tmp
          mountPath: /tmp

        # Resource limits
        # Scale: 100 CCU baseline, adjust for higher scales
        resources:
          requests:
            cpu: 500m      # 100 CCU baseline
            memory: 256Mi
          limits:
            cpu: 2000m     # Allow burst for high load
            memory: 512Mi

        # Liveness probe (checks if HAProxy process is running)
        livenessProbe:
          httpGet:
            path: /stats
            port: stats
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        # Readiness probe (checks if HAProxy is ready to route traffic)
        readinessProbe:
          httpGet:
            path: /stats
            port: stats
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

        # Startup probe (allow slow startup)
        startupProbe:
          httpGet:
            path: /stats
            port: stats
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 12  # 60 seconds max startup time

        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  # Gracefully drain connections before shutdown
                  echo "set server <backend>/<server> state maint" | socat stdio /var/run/haproxy.sock || true
                  sleep 10

      volumes:
      # HAProxy configuration
      - name: haproxy-config
        configMap:
          name: haproxy-config
          items:
          - key: haproxy.cfg
            path: haproxy.cfg

      # Runtime directory for stats socket
      - name: run
        emptyDir: {}

      # Temporary directory
      - name: tmp
        emptyDir: {}

      # Pod anti-affinity (spread HAProxy pods across nodes)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - haproxy
                - key: component
                  operator: In
                  values:
                  - routing-layer
              topologyKey: kubernetes.io/hostname

      # Topology spread constraints (even distribution across zones)
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: haproxy
            component: routing-layer

---
# ============================================================================
# HAPROXY SERVICE (ClusterIP)
# ============================================================================
apiVersion: v1
kind: Service
metadata:
  name: haproxy
  namespace: matrix
  labels:
    app: haproxy
    component: routing-layer
spec:
  type: ClusterIP
  selector:
    app: haproxy
    component: routing-layer
  ports:
  - name: http
    port: 8008
    targetPort: http
    protocol: TCP
  - name: stats
    port: 8404
    targetPort: stats
    protocol: TCP
  sessionAffinity: None

---
# ============================================================================
# SERVICE MONITOR (Prometheus Operator)
# ============================================================================
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: haproxy
  namespace: matrix
  labels:
    app: haproxy
    component: routing-layer
spec:
  selector:
    matchLabels:
      app: haproxy
      component: routing-layer
  endpoints:
  - port: stats
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
# ============================================================================
# POD DISRUPTION BUDGET
# ============================================================================
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: haproxy
  namespace: matrix
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: haproxy
      component: routing-layer

---
# ============================================================================
# HORIZONTAL POD AUTOSCALER (Optional)
# ============================================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: haproxy
  namespace: matrix
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: haproxy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # Scale based on CPU usage
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Scale based on memory usage
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50  # Scale down max 50% of pods at once
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60   # Wait 1 min before scaling up
      policies:
      - type: Percent
        value: 100  # Scale up max 100% (double) at once
        periodSeconds: 60
