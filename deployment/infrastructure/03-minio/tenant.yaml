---
# MinIO Tenant - Distributed Object Storage
# Provides S3-compatible storage for Synapse media and PostgreSQL backups
# Erasure Coding EC:4 (4 data + 4 parity shards)
apiVersion: minio.min.io/v2
kind: Tenant
metadata:
  name: matrix-minio
  namespace: matrix
  labels:
    app.kubernetes.io/name: minio
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: matrix-stack
  annotations:
    prometheus.io/path: /minio/v2/metrics/cluster
    prometheus.io/port: "9000"
    prometheus.io/scrape: "true"

spec:
  ## S3 Features
  features:
    bucketDNS: false
    domains: {}

  ## Configuration secret
  configuration:
    name: minio-config

  ## User credentials
  users:
    - name: minio-credentials

  ## Auto-create buckets on deployment
  buckets:
    - name: synapse-media
      objectLock: false
    - name: synapse-media-li
      objectLock: false
    - name: postgresql-backups
      objectLock: false

  ## MinIO image
  image: quay.io/minio/minio:RELEASE.2024-11-07T00-52-20Z
  imagePullSecret: {}

  ## Mount configuration
  mountPath: /export
  subPath: ""

  ## Pod management - Parallel for faster deployments
  podManagementPolicy: Parallel

  ## Service account (operator creates one automatically if not specified)
  serviceAccountName: ""

  ## MinIO Pool Configuration
  ## For EC:4 erasure coding, we need minimum 8 drives (4 data + 4 parity)
  ## Configuration: 4 servers × 2 volumes = 8 drives
  pools:
    - name: pool-0
      ## 4 servers minimum for distributed mode
      servers: 4

      ## 2 volumes per server = 8 total drives
      ## This gives us EC:4 (4 data + 4 parity shards)
      volumesPerServer: 2

      ## Pod anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    v1.min.io/tenant: matrix-minio
                    v1.min.io/pool: pool-0
                topologyKey: kubernetes.io/hostname

      ## Resources per pod
      resources:
        requests:
          memory: 2Gi
          cpu: 1
        limits:
          memory: 4Gi
          cpu: 2

      ## Volume configuration
      ## Each volume is 500Gi, so:
      ## - Total raw storage: 4 servers × 2 volumes × 500Gi = 4Ti
      ## - Usable storage with EC:4: ~2Ti (50% efficiency)
      volumeClaimTemplate:
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 500Gi
          storageClassName: standard

      ## Security context
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        fsGroup: 1000
        fsGroupChangePolicy: "OnRootMismatch"

      ## Container security context
      containerSecurityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault

  ## TLS - Auto-generated certificates
  requestAutoCert: true

  ## Prometheus monitoring
  prometheusOperator:
    labels:
      app: minio-metrics

---
# PodDisruptionBudget for MinIO
# Ensures at least 3 out of 4 pods are available during updates
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: matrix-minio-pdb
  namespace: matrix
  labels:
    app.kubernetes.io/name: minio
spec:
  minAvailable: 3
  selector:
    matchLabels:
      v1.min.io/tenant: matrix-minio
