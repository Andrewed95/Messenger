# ============================================================================
# HAProxy Configuration for Matrix/Synapse
# Version: 2.0
# Purpose: Intelligent routing layer for Synapse workers
# Scale-aware: Adjust server-template counts based on SCALING-GUIDE.md
# ============================================================================

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================
global
    # Logging to stdout (captured by Kubernetes)
    log stdout format raw local0 info

    # Maximum concurrent connections
    # Scale: 100 CCU=10k, 1K CCU=20k, 5K CCU=30k, 10K CCU=40k, 20K CCU=50k
    maxconn 20000

    # Performance tuning
    tune.ssl.default-dh-param 2048
    tune.bufsize 32768
    tune.maxrewrite 8192

    # Stats socket for monitoring and runtime API
    stats socket /var/run/haproxy.sock mode 660 level admin expose-fd listeners
    stats timeout 30s

    # Daemon mode (run in background)
    daemon

# ============================================================================
# DEFAULTS
# ============================================================================
defaults
    log global
    mode http
    option httplog
    option dontlognull
    option http-server-close
    option forwardfor except 127.0.0.0/8

    # Timeouts
    timeout connect 10s        # Time to establish connection to backend
    timeout client 90s         # Client inactivity timeout
    timeout server 90s         # Server inactivity timeout
    timeout http-request 10s   # Time to receive complete HTTP request
    timeout http-keep-alive 10s
    timeout queue 30s          # Max time in queue waiting for connection slot
    timeout tunnel 3600s       # Timeout for WebSocket/long-polling connections

    # Error handling
    errorfile 400 /usr/local/etc/haproxy/errors/400.http
    errorfile 403 /usr/local/etc/haproxy/errors/403.http
    errorfile 408 /usr/local/etc/haproxy/errors/408.http
    errorfile 500 /usr/local/etc/haproxy/errors/500.http
    errorfile 502 /usr/local/etc/haproxy/errors/502.http
    errorfile 503 /usr/local/etc/haproxy/errors/503.http
    errorfile 504 /usr/local/etc/haproxy/errors/504.http

# ============================================================================
# DNS RESOLVERS
# ============================================================================
resolvers kubedns
    # Kubernetes DNS service (kube-dns or CoreDNS)
    nameserver dns1 10.96.0.10:53

    # DNS resolution settings
    accepted_payload_size 8192
    hold valid 10s      # Cache valid responses for 10s
    hold obsolete 30s   # Keep obsolete records for 30s during resolution
    hold nx 10s         # Cache NXDOMAIN for 10s
    hold timeout 10s    # Retry failed queries after 10s
    hold refused 10s

    # Resolution retries
    resolve_retries 3
    timeout resolve 5s
    timeout retry 1s

# ============================================================================
# FRONTEND - STATS & METRICS
# ============================================================================
frontend stats
    bind :8404

    # Prometheus metrics endpoint
    http-request use-service prometheus-exporter if { path /metrics }

    # HAProxy stats page
    stats enable
    stats uri /stats
    stats refresh 10s
    stats show-legends
    stats show-node
    stats admin if TRUE

# ============================================================================
# FRONTEND - MATRIX HTTP TRAFFIC
# ============================================================================
frontend matrix-http
    bind :8008

    # Request logging
    option httplog
    option forwardfor

    # Extract access token for sync request hashing
    # From Authorization header: "Bearer mas_v1_abc123..."
    acl has_auth_header req.hdr(Authorization) -m found
    http-request set-header X-Access-Token %[req.hdr(Authorization),word(2,' ')] if has_auth_header

    # Or from query parameter: ?access_token=mas_v1_abc123...
    acl has_access_token urlp(access_token) -m found
    http-request set-header X-Access-Token %[urlp(access_token)] if has_access_token

    # Extract room ID for event creation hashing
    # Path: /_matrix/client/*/rooms/{room_id}/send/*
    http-request set-header X-Matrix-Room %[path,field(6,/)] if { path_reg ^/_matrix/client/.*/rooms/.*/send }
    http-request set-header X-Matrix-Room %[path,field(6,/)] if { path_reg ^/_matrix/client/.*/rooms/.*/state }

    # ========================================================================
    # ROUTING ACLs (Access Control Lists)
    # ========================================================================

    # Sync requests (long-polling, high volume)
    acl is_sync path_beg /_matrix/client/ path_end /sync
    acl is_initial_sync path_beg /_matrix/client/ path_sub /initialSync
    acl is_events path_beg /_matrix/client/ path_sub /events

    # Room messages and timeline
    acl is_messages path_reg ^/_matrix/client/.*/rooms/.*/messages
    acl is_context path_reg ^/_matrix/client/.*/rooms/.*/context/
    acl is_relations path_reg ^/_matrix/client/.*/rooms/.*/relations/

    # User filters (used by /sync)
    acl is_filter path_reg ^/_matrix/client/.*/user/.*/filter

    # To-device messages and E2E encryption
    acl is_to_device path_beg /_matrix/client/ path_sub /sendToDevice/
    acl is_keys_upload path_reg ^/_matrix/client/.*/keys/upload
    acl is_keys_query path_reg ^/_matrix/client/.*/keys/query
    acl is_keys_claim path_reg ^/_matrix/client/.*/keys/claim
    acl is_key_changes path_reg ^/_matrix/client/.*/keys/changes

    # Event creation and room operations
    acl is_send_event path_reg ^/_matrix/client/.*/rooms/.*/send/
    acl is_send_state path_reg ^/_matrix/client/.*/rooms/.*/state/
    acl is_create_room path_beg /_matrix/client/ path_sub /createRoom
    acl is_join_room path_reg ^/_matrix/client/.*/join/
    acl is_invite path_reg ^/_matrix/client/.*/rooms/.*/invite
    acl is_leave path_reg ^/_matrix/client/.*/rooms/.*/leave
    acl is_kick path_reg ^/_matrix/client/.*/rooms/.*/kick
    acl is_ban path_reg ^/_matrix/client/.*/rooms/.*/ban
    acl is_forget path_reg ^/_matrix/client/.*/rooms/.*/forget

    # Presence
    acl is_presence path_beg /_matrix/client/ path_sub /presence/

    # Typing indicators
    acl is_typing path_reg ^/_matrix/client/.*/rooms/.*/typing/

    # Read receipts
    acl is_receipt path_reg ^/_matrix/client/.*/rooms/.*/receipt/
    acl is_read_markers path_reg ^/_matrix/client/.*/rooms/.*/read_markers

    # Account data
    acl is_account_data path_reg ^/_matrix/client/.*/user/.*/account_data
    acl is_room_account_data path_reg ^/_matrix/client/.*/user/.*/rooms/.*/account_data

    # Push rules
    acl is_push_rules path_beg /_matrix/client/ path_sub /pushrules/

    # Media repository
    acl is_media_upload path_beg /_matrix/media/ path_sub /upload
    acl is_media_download path_beg /_matrix/media/ path_sub /download/
    acl is_media_thumbnail path_beg /_matrix/media/ path_sub /thumbnail/
    acl is_media_preview path_beg /_matrix/media/ path_sub /preview_url
    acl is_media_config path_beg /_matrix/media/ path_sub /config

    # Federation (inbound)
    acl is_federation_send path_beg /_matrix/federation/ path_sub /send/
    acl is_federation_event path_reg ^/_matrix/federation/.*/event/
    acl is_federation_state path_reg ^/_matrix/federation/.*/state/
    acl is_federation_state_ids path_reg ^/_matrix/federation/.*/state_ids/
    acl is_federation_backfill path_reg ^/_matrix/federation/.*/backfill/
    acl is_federation_query path_reg ^/_matrix/federation/.*/query/
    acl is_federation_make_join path_reg ^/_matrix/federation/.*/make_join/
    acl is_federation_make_leave path_reg ^/_matrix/federation/.*/make_leave/
    acl is_federation_send_join path_reg ^/_matrix/federation/.*/send_join/
    acl is_federation_send_leave path_reg ^/_matrix/federation/.*/send_leave/
    acl is_federation_invite path_reg ^/_matrix/federation/.*/invite/

    # Federation (all)
    acl is_federation path_beg /_matrix/federation/

    # User directory search
    acl is_user_dir path_beg /_matrix/client/ path_sub /user_directory/search

    # ========================================================================
    # ROUTING RULES (Order matters - most specific first)
    # ========================================================================

    # Media repository → media-repo-workers
    use_backend media-repo-workers if is_media_upload or is_media_download or is_media_thumbnail or is_media_preview or is_media_config

    # Sync and timeline → sync-workers (high volume, token-based hashing)
    use_backend sync-workers if is_sync or is_initial_sync or is_events or is_messages or is_context or is_filter or is_relations

    # Event creation → event-creator-workers (room-based hashing for ordering)
    use_backend event-creator-workers if is_send_event or is_send_state or is_create_room or is_join_room or is_invite or is_leave or is_kick or is_ban or is_forget

    # To-device messages and E2E keys → to-device-workers
    use_backend to-device-workers if is_to_device or is_keys_upload or is_keys_query or is_keys_claim or is_key_changes

    # Presence → presence-workers
    use_backend presence-workers if is_presence

    # Typing indicators → typing-workers
    use_backend typing-workers if is_typing

    # Read receipts → receipts-workers
    use_backend receipts-workers if is_receipt or is_read_markers

    # Account data → account-data-workers
    use_backend account-data-workers if is_account_data or is_room_account_data

    # Push rules → push-rules-workers
    use_backend push-rules-workers if is_push_rules

    # User directory → user-dir-workers
    use_backend user-dir-workers if is_user_dir

    # Federation (inbound) → federation-inbound-workers (origin-based hashing)
    use_backend federation-inbound-workers if is_federation

    # Default: generic-workers (handles all other endpoints)
    default_backend generic-workers

# ============================================================================
# BACKEND - SYNC WORKERS
# Token-based hashing (same user → same worker)
# Scale: 100 CCU=2, 1K CCU=4, 5K CCU=6, 10K CCU=8, 20K CCU=12
# ============================================================================
backend sync-workers
    balance hdr(X-Access-Token)
    hash-type consistent

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    # Service discovery via DNS SRV
    # Adjust count based on scale (see SCALING-GUIDE.md)
    server-template sync 4 _synapse-sync._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 2000

    # Fallback to generic workers
    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    # Ultimate fallback to main process
    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - EVENT CREATOR WORKERS
# Room-based hashing (same room → same worker for event ordering)
# Scale: 100 CCU=1, 1K CCU=2, 5K CCU=3, 10K CCU=4, 20K CCU=6
# ============================================================================
backend event-creator-workers
    balance hdr(X-Matrix-Room)
    hash-type consistent

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    # Adjust count based on scale
    server-template creator 2 _synapse-event-creator._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    # Fallback to generic workers
    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - TO-DEVICE WORKERS
# Round-robin (to-device messages, E2E encryption keys)
# Scale: 100 CCU=1, 1K CCU=2, 5K CCU=3, 10K CCU=4, 20K CCU=6
# ============================================================================
backend to-device-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template todevice 2 _synapse-to-device._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - MEDIA REPOSITORY WORKERS
# Round-robin (media uploads and downloads)
# Scale: 100 CCU=1, 1K CCU=2, 5K CCU=3, 10K CCU=4, 20K CCU=6
# ============================================================================
backend media-repo-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    # Higher maxconn for media transfers
    server-template media 2 _synapse-media-repo._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 500

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - FEDERATION INBOUND WORKERS
# Source IP hashing (same origin server → same worker)
# Scale: 100 CCU=1, 1K CCU=2, 5K CCU=3, 10K CCU=4, 20K CCU=6
# ============================================================================
backend federation-inbound-workers
    balance source
    hash-type consistent

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template fed-in 2 _synapse-federation-inbound._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - PRESENCE WORKERS
# Round-robin
# Scale: 100 CCU=0 (disabled), 1K CCU=1, 5K CCU=2, 10K CCU=3, 20K CCU=4
# ============================================================================
backend presence-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template presence 1 _synapse-presence._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - TYPING WORKERS
# Round-robin
# Scale: 100 CCU=0 (use event-persister), 1K CCU=1, 5K CCU=2, 10K CCU=3, 20K CCU=4
# ============================================================================
backend typing-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template typing 1 _synapse-typing._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - RECEIPTS WORKERS
# Round-robin
# Scale: 100 CCU=0 (use event-persister), 1K CCU=1, 5K CCU=1, 10K CCU=2, 20K CCU=3
# ============================================================================
backend receipts-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template receipts 1 _synapse-receipts._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - ACCOUNT DATA WORKERS
# Round-robin
# Scale: 100 CCU=0 (use generic), 1K CCU=1, 5K CCU=1, 10K CCU=2, 20K CCU=3
# ============================================================================
backend account-data-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template accountdata 1 _synapse-account-data._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - PUSH RULES WORKERS
# Round-robin
# Scale: 100 CCU=0 (use generic), 1K CCU=1, 5K CCU=1, 10K CCU=1, 20K CCU=2
# ============================================================================
backend push-rules-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template pushrules 1 _synapse-push-rules._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 500

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - USER DIRECTORY WORKERS
# Round-robin
# Scale: 100 CCU=0 (use generic), 1K CCU=1, 5K CCU=1, 10K CCU=1, 20K CCU=2
# ============================================================================
backend user-dir-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template userdir 1 _synapse-user-dir._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 500

    server-template generic-fallback 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup

    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - GENERIC WORKERS (Default/Fallback)
# Round-robin (handles all endpoints not routed elsewhere)
# Scale: 100 CCU=2, 1K CCU=2, 5K CCU=3, 10K CCU=4, 20K CCU=6
# ============================================================================
backend generic-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    server-template generic 2 _synapse-generic._tcp.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    # Fallback to main process if all generic workers down
    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup
