# ============================================================================
# HAProxy Configuration for Matrix/Synapse
# Purpose: Intelligent routing layer for Synapse workers
# Scale-aware: Adjust server-template counts based on SCALING-GUIDE.md
# ============================================================================

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================
global
    # Logging to stdout (captured by Kubernetes)
    log stdout format raw local0 info

    # Maximum concurrent connections
    # Scale: 100 CCU=10k, 1K CCU=20k, 5K CCU=30k, 10K CCU=40k, 20K CCU=50k
    maxconn 20000

    # Performance tuning
    tune.ssl.default-dh-param 2048
    tune.bufsize 32768
    tune.maxrewrite 8192

    # Stats socket for monitoring and runtime API
    stats socket /var/run/haproxy.sock mode 660 level admin expose-fd listeners
    stats timeout 30s

# ============================================================================
# DEFAULTS
# ============================================================================
defaults
    log global
    mode http
    option httplog
    option dontlognull
    option http-server-close
    option forwardfor except 127.0.0.0/8

    # Timeouts
    timeout connect 10s        # Time to establish connection to backend
    timeout client 90s         # Client inactivity timeout
    timeout server 90s         # Server inactivity timeout
    timeout http-request 10s   # Time to receive complete HTTP request
    timeout http-keep-alive 10s
    timeout queue 30s          # Max time in queue waiting for connection slot
    timeout tunnel 3600s       # Timeout for WebSocket/long-polling connections

    # Error handling
    errorfile 400 /usr/local/etc/haproxy/errors/400.http
    errorfile 403 /usr/local/etc/haproxy/errors/403.http
    errorfile 408 /usr/local/etc/haproxy/errors/408.http
    errorfile 500 /usr/local/etc/haproxy/errors/500.http
    errorfile 502 /usr/local/etc/haproxy/errors/502.http
    errorfile 503 /usr/local/etc/haproxy/errors/503.http
    errorfile 504 /usr/local/etc/haproxy/errors/504.http

# ============================================================================
# DNS RESOLVERS
# ============================================================================
resolvers kubedns
    # Kubernetes DNS service (kube-dns or CoreDNS)
    nameserver dns1 10.96.0.10:53

    # DNS resolution settings
    accepted_payload_size 8192
    hold valid 10s      # Cache valid responses for 10s
    hold obsolete 30s   # Keep obsolete records for 30s during resolution
    hold nx 10s         # Cache NXDOMAIN for 10s
    hold timeout 10s    # Retry failed queries after 10s
    hold refused 10s

    # Resolution retries
    resolve_retries 3
    timeout resolve 5s
    timeout retry 1s

# ============================================================================
# FRONTEND - STATS & METRICS
# ============================================================================
frontend stats
    bind :8404

    # Prometheus metrics endpoint
    http-request use-service prometheus-exporter if { path /metrics }

    # HAProxy stats page
    stats enable
    stats uri /stats
    stats refresh 10s
    stats show-legends
    stats show-node
    stats admin if TRUE

# ============================================================================
# FRONTEND - MATRIX HTTP TRAFFIC
# ============================================================================
frontend matrix-http
    bind :8008

    # Request logging
    option httplog
    option forwardfor

    # Extract access token for sync request hashing
    # From Authorization header: "Bearer mas_v1_abc123..."
    acl has_auth_header req.hdr(Authorization) -m found
    http-request set-header X-Access-Token %[req.hdr(Authorization),word(2,' ')] if has_auth_header

    # Or from query parameter: ?access_token=mas_v1_abc123...
    acl has_access_token urlp(access_token) -m found
    http-request set-header X-Access-Token %[urlp(access_token)] if has_access_token

    # ========================================================================
    # ROUTING ACLs (Access Control Lists)
    # ========================================================================

    # Sync requests (long-polling, high volume)
    acl is_sync path_beg /_matrix/client/ path_end /sync
    acl is_initial_sync path_beg /_matrix/client/ path_sub /initialSync
    acl is_events path_beg /_matrix/client/ path_sub /events

    # All other Matrix endpoints handled by generic workers
    acl is_matrix path_beg /_matrix/
    acl is_synapse_admin path_beg /_synapse/admin/
    acl is_well_known path_beg /.well-known/matrix/

    # ========================================================================
    # ROUTING RULES
    # ========================================================================

    # Sync endpoints → sync-workers (token-based hashing for session affinity)
    use_backend sync-workers if is_sync or is_initial_sync or is_events

    # All other Matrix traffic → generic-workers (handles everything else)
    use_backend generic-workers if is_matrix or is_synapse_admin or is_well_known

    # Default: generic-workers
    default_backend generic-workers

# ============================================================================
# BACKEND - SYNC WORKERS
# Token-based hashing (same user → same worker)
# Scale: 100 CCU=2, 1K CCU=4, 5K CCU=8, 10K CCU=12, 20K CCU=18
# ============================================================================
backend sync-workers
    balance hdr(X-Access-Token)
    hash-type consistent

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    # Service discovery via DNS SRV
    # Uses existing headless service: synapse-sync-worker.matrix.svc.cluster.local
    # Adjust count based on scale (see SCALING-GUIDE.md)
    server-template sync 8 _http._tcp.synapse-sync-worker.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 2000

    # Fallback to generic workers if all sync workers down
    server-template generic-fallback 4 _http._tcp.synapse-generic-worker.matrix.svc.cluster.local \
        resolvers kubedns init-addr none check backup maxconn 1000

    # Ultimate fallback to main process
    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# BACKEND - GENERIC WORKERS
# Round-robin (handles all non-sync endpoints)
# Scale: 100 CCU=2, 1K CCU=2, 5K CCU=4, 10K CCU=6, 20K CCU=8
# ============================================================================
backend generic-workers
    balance roundrobin

    option httpchk GET /_matrix/client/versions
    http-check expect status 200

    option http-keep-alive
    option forwardfor

    # Service discovery via DNS SRV
    # Uses existing headless service: synapse-generic-worker.matrix.svc.cluster.local
    server-template generic 4 _http._tcp.synapse-generic-worker.matrix.svc.cluster.local \
        resolvers kubedns \
        init-addr none \
        check inter 10s fall 3 rise 2 \
        maxconn 1000

    # Fallback to main process if all generic workers down
    server synapse-main synapse-main.matrix.svc.cluster.local:8008 check backup

# ============================================================================
# NOTES ON ARCHITECTURE
# ============================================================================
# This configuration uses a simplified worker architecture:
#
# - Sync Workers: Handle /sync endpoints (long-polling, session affinity)
#   - Token-based consistent hashing ensures same user → same worker
#   - Most resource-intensive due to long-lived connections
#   - Port 8083 (configured in worker deployments)
#
# - Generic Workers: Handle ALL other Matrix endpoints
#   - Client API (except /sync)
#   - Federation API (inbound)
#   - Media API
#   - Admin API
#   - Port 8081 (configured in worker deployments)
#
# - Background Workers (not routed through HAProxy):
#   - Event Persisters: Database write optimization
#   - Federation Senders: Outbound federation traffic
#   - Only communicate via replication protocol (port 9093)
#
# Health Checks:
# - HAProxy checks /_matrix/client/versions on each worker
# - Only routes to healthy workers (check inter 10s fall 3 rise 2)
# - Automatic fallback to main process if all workers down
#
# Service Discovery:
# - Uses Kubernetes DNS SRV records for automatic worker discovery
# - Format: _http._tcp.<service-name>.<namespace>.svc.cluster.local
# - Dynamically discovers worker pods as they scale up/down
# - No hardcoded IP addresses
#
# Scaling:
# - Adjust server-template count to match your worker replicas
# - See SCALING-GUIDE.md for worker counts at different scales
# - HAProxy will discover all pods matching the service selector
#
# Future Expansion:
# - To add specialized workers (event-creator, media-repo, etc.):
#   1. Create worker deployments with appropriate labels
#   2. Create headless services for service discovery
#   3. Add backend definitions in this config
#   4. Add routing ACLs and use_backend rules
#   5. Follow patterns from Element's ess-helm for routing logic
# ============================================================================
