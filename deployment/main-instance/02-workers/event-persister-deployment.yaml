# Event Persister Worker Deployment
# Dedicated workers for writing events to the database
# Critical for write performance - handles event persistence
#
# Scale: 2-8 replicas based on write load
# NOTE: Changing replica count requires restarting ALL Synapse processes

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: synapse-event-persister
  namespace: matrix
  labels:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: event-persister
    app.kubernetes.io/instance: main
    matrix.instance: main
spec:
  serviceName: synapse-event-persister
  # Increased from 2 to 4 for better write distribution at scale
  # With HPA, can scale up to 12 replicas based on CPU load
  # NOTE: Scaling StatefulSet workers requires updating instance_map and restarting all Synapse processes
  replicas: 4  # Increased minimum for high-scale deployment
  podManagementPolicy: Parallel

  selector:
    matchLabels:
      app.kubernetes.io/name: synapse
      app.kubernetes.io/component: event-persister

  updateStrategy:
    type: RollingUpdate

  template:
    metadata:
      labels:
        app.kubernetes.io/name: synapse
        app.kubernetes.io/component: event-persister
        app.kubernetes.io/type: worker
        app.kubernetes.io/instance: main
        matrix.instance: main
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/_synapse/metrics"

    spec:
      securityContext:
        runAsUser: 991
        runAsGroup: 991
        fsGroup: 991
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: event-persister
                topologyKey: kubernetes.io/hostname

      initContainers:
        - name: generate-worker-config
          image: matrixdotorg/synapse:v1.119.0
          command:
            - sh
            - -c
            - |
              # Create worker-specific config
              cat > /config/worker.yaml <<EOF
              worker_app: synapse.app.generic_worker
              worker_name: event_persister_${HOSTNAME}

              # Replication
              worker_replication_host: synapse-main-0.synapse-main.matrix.svc.cluster.local
              worker_replication_http_port: 9093
              worker_replication_http_tls: false

              # Event persister workers only need replication listener
              worker_listeners:
                - type: http
                  port: 9093
                  resources:
                    - names: [replication]

                - type: metrics
                  port: 9090
                  resources:
                    - names: [metrics]

              # Worker log config
              worker_log_config: /config/log.yaml
              EOF

              # Process shared config
              envsubst < /config-template/homeserver.yaml > /config/homeserver.yaml
              cp /config-template/log.yaml /config/log.yaml
              cp /config-template/signing.key /config/signing.key
              chown -R 991:991 /config
              chmod 600 /config/signing.key

              echo "Event persister worker configured for ${HOSTNAME}"
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: matrix-postgresql-app
                  key: password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: REDIS_PASSWORD
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: S3_ACCESS_KEY
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: S3_SECRET_KEY
            - name: REPLICATION_SECRET
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: REPLICATION_SECRET
            - name: MACAROON_SECRET
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: MACAROON_SECRET
            - name: REGISTRATION_SECRET
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: REGISTRATION_SECRET
            - name: FORM_SECRET
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: FORM_SECRET
            - name: TURN_SECRET
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: TURN_SECRET
            - name: KEY_VAULT_API_KEY
              valueFrom:
                secretKeyRef:
                  name: synapse-secrets
                  key: KEY_VAULT_API_KEY
          volumeMounts:
            - name: config-template
              mountPath: /config-template
            - name: config
              mountPath: /config
            - name: signing-key
              mountPath: /config-template/signing.key
              subPath: signing.key

      containers:
        - name: worker
          image: matrixdotorg/synapse:v1.119.0
          imagePullPolicy: IfNotPresent

          command:
            - python
            - -m
            - synapse.app.generic_worker
            - --config-path=/config/homeserver.yaml
            - --config-path=/config/worker.yaml

          ports:
            - name: replication
              containerPort: 9093
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP

          env:
            - name: SYNAPSE_WORKER
              value: "synapse.app.generic_worker"
            - name: SYNAPSE_WORKER_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

          volumeMounts:
            - name: config
              mountPath: /config
            - name: tmp
              mountPath: /tmp

          # Event persisters are write-heavy, need more resources
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"

          # Health probes via metrics endpoint
          livenessProbe:
            httpGet:
              path: /_synapse/metrics
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /_synapse/metrics
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 2

          startupProbe:
            httpGet:
              path: /_synapse/metrics
              port: 9090
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 20

          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 991
            runAsGroup: 991
            capabilities:
              drop:
                - ALL

      volumes:
        - name: config-template
          configMap:
            name: synapse-config
        - name: config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: signing-key
          secret:
            secretName: synapse-secrets
            items:
              - key: signing.key
                path: signing.key
                mode: 0600

---
# Headless service for event persisters
apiVersion: v1
kind: Service
metadata:
  name: synapse-event-persister
  namespace: matrix
  labels:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: event-persister
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: event-persister
  ports:
    - name: replication
      port: 9093
      targetPort: 9093
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP

---
# PodDisruptionBudget - keep at least 2 event persisters running (increased from 1)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: synapse-event-persister-pdb
  namespace: matrix
  labels:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: event-persister
spec:
  minAvailable: 2  # Increased from 1 to ensure write capacity during node drains
  selector:
    matchLabels:
      app.kubernetes.io/name: synapse
      app.kubernetes.io/component: event-persister

---
# HorizontalPodAutoscaler - Auto-scale event persisters based on CPU
# NOTE: Scaling event-persisters requires updating instance_map in Synapse config
# and restarting ALL Synapse processes. This HPA provides automation but
# requires coordinated deployment. See docs/OPERATIONS-UPDATE-GUIDE.md Section 9.
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: synapse-event-persister-hpa
  namespace: matrix
  labels:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: event-persister
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: synapse-event-persister
  minReplicas: 4  # Matches StatefulSet replicas
  maxReplicas: 12  # Maximum for high-scale deployment (20K CCU)
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale up at 70% CPU to prevent saturation
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120  # Wait 2 minutes before scaling up
      policies:
        - type: Pods
          value: 2  # Add 2 pods at a time
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Pods
          value: 1  # Remove 1 pod at a time slowly
          periodSeconds: 120  # Every 2 minutes
