# Synapse ConfigMap
# Contains homeserver.yaml and log.yaml configuration files
# These are mounted into the Synapse pods at /config/

apiVersion: v1
kind: ConfigMap
metadata:
  name: synapse-config
  namespace: matrix
  labels:
    app.kubernetes.io/name: synapse
    app.kubernetes.io/component: homeserver
    matrix.instance: main
data:
  homeserver.yaml: |
    # Synapse Homeserver Configuration
    # Integrates with Phase 1 infrastructure (PostgreSQL, Redis, MinIO)
    # Production Matrix deployment with LI capabilities and worker support

    # ==================== CORE SETTINGS ====================

    server_name: "matrix.example.com"
    public_baseurl: "https://matrix.example.com"
    pid_file: /data/homeserver.pid
    web_client_location: "https://chat.example.com"

    # ==================== LISTENERS ====================

    listeners:
      # Client and Federation API (proxied via HAProxy)
      - port: 8008
        tls: false
        type: http
        x_forwarded: true
        bind_addresses: ['0.0.0.0']
        resources:
          - names: [client, federation]
            compress: false

      # Replication endpoint for workers (internal only)
      # SECURITY: Binds to all pod interfaces since workers connect via Kubernetes DNS.
      # Network isolation is the organization's responsibility (per CLAUDE.md 7.4).
      # The worker_replication_secret provides authentication for replication traffic.
      - port: 9093
        type: http
        bind_addresses: ['0.0.0.0']
        resources:
          - names: [replication]

      # Metrics endpoint
      - port: 9090
        type: metrics
        bind_addresses: ['0.0.0.0']

    # ==================== DATABASE ====================

    database:
      name: psycopg2
      args:
        user: synapse
        password: ${DB_PASSWORD}
        database: matrix
        host: matrix-postgresql-rw.matrix.svc.cluster.local
        port: 5432
        sslmode: require
        cp_min: 5
        cp_max: 8  # Conservative: ~50 workers max * 8 = 400 connections (safely under 500 limit)
        # Worker count at max HPA: main(1) + synchrotron(16) + generic(16) + media(2) +
        # event-persister(4) + federation(2) + typing(2) + todevice(2) + receipts(2) + presence(2) = 49

    # ==================== REDIS & WORKERS ====================

    redis:
      enabled: true
      host: redis.matrix.svc.cluster.local
      port: 6379
      password: ${REDIS_PASSWORD}

    # Worker replication secret for security
    worker_replication_secret: "${REPLICATION_SECRET}"

    # Instance mapping for workers
    # Workers will auto-register via replication protocol

    # Federation sender workers configuration
    # These workers handle outbound federation traffic
    send_federation: false  # Main process should NOT send federation
    federation_sender_instances:
      - synapse-federation-sender-0
      - synapse-federation-sender-1

    # Stream writers configuration
    # CRITICAL: These workers handle high-frequency writes offloading main process
    # Changed from main process handling to dedicated stream writers for performance
    # At 20K CCU: typing ~100s/sec, to_device ~10K/sec, receipts ~500/sec, presence ~666/sec
    stream_writers:
      events:
        - synapse-event-persister-0
        - synapse-event-persister-1
        - synapse-event-persister-2  # Increased from 2 to 4 replicas
        - synapse-event-persister-3
      typing:
        - synapse-typing-writer-0  # Dedicated typing indicator workers
        - synapse-typing-writer-1
      to_device:
        - synapse-todevice-writer-0  # Dedicated E2EE key exchange workers
        - synapse-todevice-writer-1
      account_data:
        - synapse-main-0  # Account data remains on main (low volume)
      receipts:
        - synapse-receipts-writer-0  # Dedicated read receipt workers
        - synapse-receipts-writer-1
      presence:
        - synapse-presence-writer-0  # Dedicated presence workers
        - synapse-presence-writer-1

    # Media repository workers configuration
    # These workers handle media upload/download
    media_instance_running_background_jobs: synapse-media-repository-0
    # Using headless service for worker discovery
    # All workers need to be in the instance_map for proper routing
    instance_map:
      synapse-main-0:
        host: synapse-main-0.synapse-main.matrix.svc.cluster.local
        port: 9093

      # Federation sender workers (StatefulSet)
      synapse-federation-sender-0:
        host: synapse-federation-sender-0.synapse-federation-sender.matrix.svc.cluster.local
        port: 9093
      synapse-federation-sender-1:
        host: synapse-federation-sender-1.synapse-federation-sender.matrix.svc.cluster.local
        port: 9093

      # Event persister workers (StatefulSet) - increased from 2 to 4
      synapse-event-persister-0:
        host: synapse-event-persister-0.synapse-event-persister.matrix.svc.cluster.local
        port: 9093
      synapse-event-persister-1:
        host: synapse-event-persister-1.synapse-event-persister.matrix.svc.cluster.local
        port: 9093
      synapse-event-persister-2:
        host: synapse-event-persister-2.synapse-event-persister.matrix.svc.cluster.local
        port: 9093
      synapse-event-persister-3:
        host: synapse-event-persister-3.synapse-event-persister.matrix.svc.cluster.local
        port: 9093

      # Stream writer workers (NEW - offload high-frequency writes from main process)
      # Typing indicator writers
      synapse-typing-writer-0:
        host: synapse-typing-writer-0.synapse-typing-writer.matrix.svc.cluster.local
        port: 9093
      synapse-typing-writer-1:
        host: synapse-typing-writer-1.synapse-typing-writer.matrix.svc.cluster.local
        port: 9093

      # To-device message writers (E2EE key exchanges)
      synapse-todevice-writer-0:
        host: synapse-todevice-writer-0.synapse-todevice-writer.matrix.svc.cluster.local
        port: 9093
      synapse-todevice-writer-1:
        host: synapse-todevice-writer-1.synapse-todevice-writer.matrix.svc.cluster.local
        port: 9093

      # Read receipt writers
      synapse-receipts-writer-0:
        host: synapse-receipts-writer-0.synapse-receipts-writer.matrix.svc.cluster.local
        port: 9093
      synapse-receipts-writer-1:
        host: synapse-receipts-writer-1.synapse-receipts-writer.matrix.svc.cluster.local
        port: 9093

      # Presence writers
      synapse-presence-writer-0:
        host: synapse-presence-writer-0.synapse-presence-writer.matrix.svc.cluster.local
        port: 9093
      synapse-presence-writer-1:
        host: synapse-presence-writer-1.synapse-presence-writer.matrix.svc.cluster.local
        port: 9093

      # Media repository workers (StatefulSet) - required for media_instance_running_background_jobs
      synapse-media-repository-0:
        host: synapse-media-repository-0.synapse-media-repository.matrix.svc.cluster.local
        port: 9093
      synapse-media-repository-1:
        host: synapse-media-repository-1.synapse-media-repository.matrix.svc.cluster.local
        port: 9093

    # Worker configuration for auto-discovery
    # Workers will connect to main process for registration
    worker_replication_host: synapse-main-0.synapse-main.matrix.svc.cluster.local
    worker_replication_http_port: 9093

    # Background tasks configuration
    # CRITICAL: Specifies which instance runs background database maintenance tasks
    # Must be an instance listed in instance_map - using main process for stability
    run_background_tasks_on: synapse-main-0

    # ==================== MEDIA STORAGE ====================

    # Media repository disabled on main process (handled by media workers)
    enable_media_repo: false
    media_store_path: "/data/media_store"

    # S3 storage provider for MinIO
    media_storage_providers:
      - module: s3_storage_provider.S3StorageProviderBackend
        store_local: true
        store_remote: true
        store_synchronous: true
        config:
          bucket: synapse-media
          endpoint_url: http://minio.matrix.svc.cluster.local:9000
          access_key_id: ${S3_ACCESS_KEY}
          secret_access_key: ${S3_SECRET_KEY}

    # Media limits
    max_upload_size: 100M
    max_image_pixels: 32M

    # ==================== LAWFUL INTERCEPT (LI) ====================

    # CRITICAL: Infinite retention for deleted messages (LI compliance)
    redaction_retention_period: null

    # LI: Disable automatic message retention to preserve all messages
    retention:
      enabled: false

    # NOTE: Recovery key storage is handled via the li.enabled endpoint below.
    # When li.enabled: true, Synapse registers the /_synapse/client/v1/li/store_key
    # endpoint (implemented in synapse/rest/client/li_proxy.py) which handles
    # RSA-encrypted recovery key forwarding to key_vault.
    # A Synapse module is NOT required - the li_proxy endpoint is built into synapse-li.

    # LI system configuration - enables key capture, endpoint protection, and session limiting
    # Per LI_IMPLEMENTATION.md Components 1, 3, and 3.5
    li:
      # Enable LI proxy endpoint for encrypted recovery key forwarding
      # When enabled, /_synapse/client/v1/li/store_key endpoint becomes available
      # Clients (element-web, element-x-android) POST RSA-encrypted recovery keys to this endpoint
      enabled: true

      # key_vault service URL (Django service storing RSA-encrypted keys)
      # Only main Synapse should access this service (network isolation is org's responsibility)
      # DO NOT change unless key_vault deployed on different host/port
      key_vault_url: "http://key-vault.matrix.svc.cluster.local:8000"

      # Endpoint Protection: Prevent non-admin users from:
      # - Forgetting rooms: POST /_matrix/client/r0/rooms/{roomId}/forget
      # - Deactivating accounts: POST /_matrix/client/r0/account/deactivate
      # When enabled, only server administrators can perform these actions
      # Regular users will receive HTTP 403 with message "Only server administrators can..."
      # Set to false ONLY for testing/debugging LI features
      endpoint_protection_enabled: true

    # Session Limiting: Maximum concurrent sessions per user across all devices
    # Enforced via synapse/handlers/li_session_limiter.py
    # Tracks sessions in /var/lib/synapse/li_session_tracking.json (file-based with locking)
    # When limit exceeded, returns HTTP 429 "Maximum concurrent sessions exceeded"
    #
    # Behavior:
    # - Applies to ALL users without exception (no admin bypass per LI requirements)
    # - Session = device login (each Element Web tab, each mobile app instance)
    # - Enforced at device registration (check_device_registered in device.py)
    # - Cleaned up hourly via sync_with_database (removes orphaned sessions)
    #
    # Recommended values:
    # - 5: Standard (typical user has phone + laptop + tablet + 2 browsers)
    # - 10: Power users (many devices/browsers)
    # - null: Unlimited (disable feature for non-LI deployments)
    max_sessions_per_user: 5

    # ==================== FEDERATION ====================

    # Federation disabled by default (per CLAUDE.md Rule 4.3, Rule 12)
    # To enable federation later:
    # 1. Change to null (allow all) or list specific domains: ["trusted.example.com"]
    # 2. Ensure TLS certificates cover federation endpoints
    # 3. Update DNS SRV records for .well-known delegation
    federation_domain_whitelist: []

    # Verify TLS certificates when federation is enabled
    federation_verify_certificates: true

    # Federation rate limiting
    rc_federation:
      window_size: 1000
      sleep_limit: 10
      sleep_delay: 500
      reject_limit: 50
      concurrent: 3

    # ==================== LIVEKIT (WebRTC SFU) ====================
    # Configuration for video/voice calling via LiveKit

    # Enable experimental MSC3401 (MatrixRTC) features
    experimental_features:
      msc3401_enabled: true  # Enable native group VoIP signaling

    # TURN server configuration for WebRTC (uses coturn)
    # CRITICAL: These URIs are sent to clients - must be externally reachable addresses!
    # Use the external domain configured for coturn (turn.matrix.example.com)
    # The coturn DaemonSet uses hostNetwork:true so clients connect directly to node IPs
    # Replace turn.matrix.example.com with your actual TURN server domain/IP
    turn_uris:
      - "turn:turn.matrix.example.com:3478?transport=udp"
      - "turn:turn.matrix.example.com:3478?transport=tcp"
      - "turns:turn.matrix.example.com:5349?transport=tcp"  # TLS variant for restrictive firewalls
    turn_shared_secret: "${TURN_SECRET}"
    turn_user_lifetime: 86400000  # 24 hours in milliseconds

    # LiveKit integration is configured in Element Web client
    # The actual LiveKit WebSocket URL is: wss://livekit.matrix.example.com
    # LiveKit API credentials are in livekit-secrets

    # ==================== SECURITY & SECRETS ====================

    macaroon_secret_key: "${MACAROON_SECRET}"
    registration_shared_secret: "${REGISTRATION_SECRET}"
    form_secret: "${FORM_SECRET}"
    signing_key_path: "/config/signing.key"

    # Trusted key servers (for federation key verification)
    # NOTE: This setting is ONLY used when federation is enabled.
    # With federation_domain_whitelist: [] (empty = federation disabled),
    # Synapse never contacts matrix.org or any external server.
    # Safe to keep as-is for intranet deployments.
    trusted_key_servers:
      - server_name: "matrix.org"

    # ==================== REGISTRATION & USERS ====================

    # Disable open registration (admin-only via shared secret)
    enable_registration: false
    enable_registration_without_verification: false

    # Allow password changes
    enable_set_displayname: true
    enable_set_avatar_url: true
    enable_3pid_changes: true

    # User directory
    user_directory:
      enabled: true
      search_all_users: false
      prefer_local_users: true

    # ==================== RATE LIMITING ====================

    rc_message:
      per_second: 0.2
      burst_count: 10

    rc_registration:
      per_second: 0.17
      burst_count: 3

    rc_login:
      address:
        per_second: 0.17
        burst_count: 3
      account:
        per_second: 0.17
        burst_count: 3
      failed_attempts:
        per_second: 0.17
        burst_count: 3

    rc_admin_redaction:
      per_second: 1
      burst_count: 50

    rc_joins:
      local:
        per_second: 0.1
        burst_count: 10
      remote:
        per_second: 0.01
        burst_count: 10

    rc_invites:
      per_room:
        per_second: 0.3
        burst_count: 10
      per_user:
        per_second: 0.003
        burst_count: 5

    # ==================== PERFORMANCE ====================

    # Event cache size (integer, not string)
    # Approximately 50,000 events cached in memory for fast access
    # Scale formula: 10K per 1K CCU (50K = ~5K CCU baseline)
    event_cache_size: 50000
    caches:
      # Global cache multiplier - 1.0 is baseline, 2.0 doubles all caches
      # Higher values use more memory but improve performance
      # Formula: 256MB RAM per 1.0 global_factor increment
      # Per config.env.example CACHE_FACTOR default: 2.0 for production
      global_factor: 2.0
      per_cache_factors:
        # Room membership cache - critical for large rooms
        get_users_in_room: 3.0

    # Background update performance
    background_updates:
      min_batch_size: 1
      default_batch_size: 100
      sleep_enabled: true


    # ==================== PUSH NOTIFICATIONS ====================
    # NOTE: Push notifications (Sygnal) intentionally NOT configured
    # Per CLAUDE.md Rule 4.3: No external services
    # Push notifications require connection to Apple/Google servers
    # Clients will use long-polling via /sync endpoint for real-time updates

    # ==================== MONITORING ====================

    enable_metrics: true
    metrics_flags:
      known_servers: true

    # Report statistics
    report_stats: false

    # ==================== LOGGING ====================

    log_config: "/config/log.yaml"

    # ==================== ADVANCED ====================

    # Presence tracking (can disable for performance)
    presence:
      enabled: true

    # Room statistics (useful for admin tools)
    stats:
      enabled: true

    # Forgotten room retention (LI: keep forever)
    forgotten_room_retention_period: null

    # Enable room list publication
    allow_public_rooms_without_auth: false
    allow_public_rooms_over_federation: true

    # URL preview settings
    url_preview_enabled: true
    url_preview_ip_range_blacklist:
      - '127.0.0.0/8'
      - '10.0.0.0/8'
      - '172.16.0.0/12'
      - '192.168.0.0/16'
      - '100.64.0.0/10'
      - '169.254.0.0/16'
      - '::1/128'
      - 'fe80::/64'
      - 'fc00::/7'

    max_spider_size: 10M

  log.yaml: |
    # Synapse Logging Configuration
    # Production-grade structured logging with appropriate levels

    version: 1

    formatters:
      precise:
        format: '%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - %(request)s - %(message)s'

    handlers:
      console:
        class: logging.StreamHandler
        formatter: precise
        stream: ext://sys.stdout

      # File handler for structured logs (optional - mainly using stdout for Kubernetes)
      file:
        class: logging.handlers.RotatingFileHandler
        formatter: precise
        filename: /data/homeserver.log
        maxBytes: 104857600  # 100MB
        backupCount: 10

    loggers:
      # Main Synapse logger
      synapse:
        level: INFO

      # HTTP requests - useful for debugging
      synapse.http.server:
        level: INFO

      # Federation - important for debugging federation issues
      synapse.federation:
        level: INFO

      # Storage layer - set to WARNING to reduce noise
      synapse.storage:
        level: WARNING

      # Replication - important for worker debugging
      synapse.replication:
        level: INFO

      # Media - track media operations
      synapse.media:
        level: INFO

      # Access logs - detailed request logging
      synapse.access.http:
        level: INFO

      # SQL - set to WARNING unless debugging database issues
      synapse.storage.SQL:
        level: WARNING

    root:
      level: INFO
      handlers: [console]

    # Disable overly verbose third-party loggers
    disable_existing_loggers: false
